{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#wsattention-prostate","title":"WSAttention-Prostate","text":"<p>Weakly Supervised Attention-Based Deep Learning for Prostate Cancer Characterization from Bi-Parametric Prostate MRI.</p> <p>WSAttention-Prostate is a two-stage deep learning pipeline that predicts clinically significant prostate cancer (csPCa) risk and PI-RADS score (2 to 5) from T2W, DWI, and ADC bpMRI sequences. The backbone is a patch based 3D Multiple-Instance Learning (MIL) model pre-trained to classify PI-RADS scores and fine-tuned to predict csPCa risk \u2014 all without requiring lesion-level annotations.</p> <p>\ud83d\udca1 GUI for real-time inference available at Hugging Face Spaces</p>"},{"location":"#key-features","title":"Key Features","text":"<ul> <li>Weakly-supervised attention \u2014 Heatmap-guided patch sampling and cosine-similarity attention loss replace the need for voxel-level labels</li> <li>3D Multiple Instance Learning \u2014 Extracts volumetric patches from MRI scans and aggregates them via transformer + attention pooling</li> <li>Two-stage pipeline \u2014 Stage 1 trains a 4-class PI-RADS classifier; Stage 2 freezes its backbone and trains a binary csPCa head</li> <li>Preprocessing \u2014 Preprocessing to minimize inter-center MRI acquisiton variability.</li> <li>End-to-end pipeline \u2014 Registration, segmentation, histogram matching, and heatmap generation, and inferencing in a single configurable pipeline</li> </ul>"},{"location":"#pipeline-overview","title":"Pipeline Overview","text":"<pre><code>%%{init: {'themeVariables': { 'fontSize': '20px' }}}%%\nflowchart LR\n    A[Raw bpMRI&lt;/br&gt;T2 + DWI + ADC] --&gt; B[Preprocessing]\n    B --&gt; C[Stage 1:&lt;/br&gt;PI-RADS Classification]\n    C --&gt; D[Stage 2:&lt;/br&gt;csPCa Prediction]\n    D --&gt; E[Risk Score + Top-5 Salient Patches]</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Getting Started \u2014 Installation and first run</li> <li>Pipeline \u2014 Full walkthrough of preprocessing, training, and evaluation</li> <li>Architecture \u2014 Model design and tensor shapes</li> <li>Configuration \u2014 YAML config reference</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#patch-extraction","title":"Patch Extraction","text":"<p>Patches are extracted using MONAI's <code>RandWeightedCropd</code> (when heatmaps are available) or <code>RandCropByPosNegLabeld</code> (without heatmaps):</p> <ul> <li>With heatmaps: The combined DWI/ADC heatmap multiplied by the prostate mask serves as the sampling weight map \u2014 regions with high DWI and low ADC are sampled more frequently</li> <li>Without heatmaps: Crops are sampled from positive (prostate) regions based on the binary mask</li> </ul> <p>Each scan yields <code>N</code> patches (default 24) of size <code>tile_size x tile_size x depth</code> (default 64x64x3).</p>"},{"location":"architecture/#tensor-shape-convention","title":"Tensor Shape Convention","text":"<p>Throughout the pipeline, tensors follow the shape <code>[B, N, C, D, H, W]</code>:</p> Dim Meaning Typical Value B Batch size 4\u20138 N Number of patches (instances) 24 C Channels (T2 + DWI + ADC) 3 D Depth (slices per patch) 3 H Patch height 64 W Patch width 64"},{"location":"architecture/#milmodel3d","title":"MILModel3D","text":"<p>The core model processes each patch independently through a CNN backbone, then aggregates patch-level features via a transformer encoder and attention pooling.</p> <pre><code>flowchart TD\n    A[\"Input [B, N, C, D, H, W]\"] --&gt; B[ResNet18-3D Backbone]\n    B --&gt; C[Transformer Encoder&lt;br/&gt;4 layers, 8 heads]\n    C --&gt; D[Attention Pooling&lt;br/&gt;512 \u2192 2048 \u2192 1]\n    D --&gt; E[\"Weighted Sum [B, 512]\"]\n    E --&gt; F[\"FC Head [B, num_classes]\"]</code></pre>"},{"location":"architecture/#forward-pass","title":"Forward Pass","text":"<ol> <li> <p>Backbone: Input is reshaped from <code>[B, N, C, D, H, W]</code> to <code>[B*N, C, D, H, W]</code> and passed through a 3D ResNet18 (with 3 input channels). The final FC layer is removed, yielding 512-dimensional features per patch.</p> </li> <li> <p>Transformer: Output from the ResNet 18 encoder is forwarded to the transformer encoder.</p> </li> <li> <p>Attention: A two-layer attention network (<code>512 \u2192 2048 \u2192 1</code> with Tanh) computes a scalar weight per patch, normalized via softmax.</p> </li> <li> <p>Classification: The attention-weighted sum of patch features produces a single <code>[B, 512]</code> vector per scan, which is projected to class logits by a linear layer.</p> </li> </ol>"},{"location":"architecture/#mil-modes","title":"MIL Modes","text":"Mode Aggregation Strategy <code>mean</code> Average logits across patches <code>max</code> Max logits across patches <code>att</code> Attention-weighted feature pooling <code>att_trans</code> Transformer encoder + attention pooling (primary mode) <code>att_trans_pyramid</code> Pyramid transformer on intermediate ResNet layers + attention <p>The default and primary mode is <code>att_trans</code>.</p>"},{"location":"architecture/#cspca_model","title":"csPCa_Model","text":"<p>Wraps a frozen <code>MILModel_3D</code> backbone and replaces the classification head:</p> <pre><code>flowchart TD\n    A[\"Input [B, N, C, D, H, W]\"] --&gt; B[\"Frozen Backbone&lt;br/&gt;(ResNet18 + Transformer)\"]\n    B --&gt; C[\"Pooled Features [B, 512]\"]\n    C --&gt; D[\"SimpleNN Head&lt;br/&gt;512 \u2192 256 \u2192 128 \u2192 1\"]\n    D --&gt; E[\"Sigmoid \u2192 csPCa Probability\"]</code></pre>"},{"location":"architecture/#simplenn","title":"SimpleNN","text":"<pre><code>Linear(512, 256) \u2192 ReLU\nLinear(256, 128) \u2192 ReLU \u2192 Dropout(0.3)\nLinear(128, 1) \u2192 Sigmoid\n</code></pre> <p>During csPCa training, the backbone's <code>net</code> (ResNet18), <code>transformer</code> are frozen. The <code>attention</code> module and <code>SimpleNN</code> head remain trainable.</p>"},{"location":"architecture/#attention-loss","title":"Attention Loss","text":"<p>During PI-RADS training with heatmaps enabled, the model uses a dual-loss objective:</p> <pre><code>total_loss = class_loss + lambda_att * attention_loss\n</code></pre> <ul> <li>Classification loss: Standard CrossEntropy on PI-RADS labels</li> <li>Attention loss: <code>1 - cosine_similarity(predicted_attention, heatmap_attention)</code><ul> <li>Heatmap-derived attention labels are computed by summing spatial heatmap values per patch, squaring for sharpness, and normalizing</li> <li>PI-RADS 2 samples get uniform attention (no expected lesion)</li> <li><code>lambda_att</code> warms up linearly from 0 to 2.0 over the first 25 epochs</li> <li>The attention predictions are computed with detached transformer outputs to avoid gradient interference with classification</li> </ul> </li> </ul>"},{"location":"configuration/","title":"Configuration","text":"<p>YAML values always override CLI defaults (<code>args.__dict__.update(config)</code>). To override a YAML value, edit the YAML file or omit the key so the CLI default is used.</p> <pre><code>python run_pirads.py --mode train --config config/config_pirads_train.yaml\n</code></pre>"},{"location":"configuration/#pi-rads-training-parameters","title":"PI-RADS Training Parameters","text":"Parameter Default Description <code>mode</code> \u2014 <code>train</code> or <code>test</code> (required) <code>config</code> \u2014 Path to YAML config file <code>data_root</code> \u2014 Root folder of T2W images <code>dataset_json</code> \u2014 Path to dataset JSON file. Format should be as specified in Getting Started <code>num_classes</code> <code>4</code> Number of output classes (PI-RADS 2\u20135) <code>mil_mode</code> <code>att_trans</code> MIL algorithm (<code>mean</code>, <code>max</code>, <code>att</code>, <code>att_trans</code>, <code>att_trans_pyramid</code>) <code>tile_count</code> <code>24</code> Number of patches per scan <code>tile_size</code> <code>64</code> Patch spatial size in pixels <code>depth</code> <code>3</code> Number of slices per patch <code>use_heatmap</code> <code>True</code> Enable heatmap-guided patch sampling <code>workers</code> <code>2</code> DataLoader workers <code>checkpoint</code> <code>None</code> Path to resume from checkpoint <code>epochs</code> <code>50</code> Max training epochs <code>early_stop</code> <code>40</code> Epochs without improvement before stopping <code>batch_size</code> <code>4</code> Scans per batch <code>optim_lr</code> <code>3e-5</code> Base learning rate <code>weight_decay</code> <code>0</code> Optimizer weight decay <code>amp</code> <code>False</code> Enable automatic mixed precision <code>val_every</code> <code>1</code> Validation frequency (epochs) <code>wandb</code> <code>False</code> Enable Weights &amp; Biases logging <code>project_name</code> <code>Classification_prostate</code> W&amp;B project name <code>run_name</code> <code>train_pirads</code> Run name for logging. If using SLURM, takes SLURM JOB_ID"},{"location":"configuration/#cspca-training-parameters","title":"csPCa Training Parameters","text":"Parameter Default Description <code>mode</code> \u2014 <code>train</code> or <code>test</code> (required) <code>config</code> \u2014 Path to YAML config file <code>data_root</code> \u2014 Root folder of images <code>dataset_json</code> \u2014 Path to dataset JSON file <code>checkpoint_pirads</code> \u2014 Path to pre-trained PI-RADS model (required for train) <code>checkpoint_cspca</code> \u2014 Path to csPCa checkpoint (required for test) <code>epochs</code> <code>30</code> Max training epochs <code>batch_size</code> <code>32</code> Scans per batch <code>optim_lr</code> <code>2e-4</code> Learning rate <code>num_seeds</code> <code>20</code> Number of random seeds for CI <p>Shared parameters (<code>num_classes</code>, <code>mil_mode</code>, <code>tile_count</code>, <code>tile_size</code>, <code>depth</code>, <code>use_heatmap</code>, <code>workers</code>, <code>val_every</code>) have the same defaults as PI-RADS.</p>"},{"location":"configuration/#preprocessing-parameters","title":"Preprocessing Parameters","text":"Parameter Default Description <code>config</code> \u2014 Path to YAML config file <code>steps</code> \u2014 Steps to execute (required, one or more) <code>t2_dir</code> \u2014 Directory of T2W images <code>dwi_dir</code> \u2014 Directory of DWI images <code>adc_dir</code> \u2014 Directory of ADC images <code>seg_dir</code> \u2014 Directory of segmentation masks <code>output_dir</code> \u2014 Output directory <code>margin</code> <code>0.2</code> Center-crop margin fraction"},{"location":"configuration/#example-yaml","title":"Example YAML","text":"PI-RADS TrainingcsPCa TrainingPreprocessing <pre><code>data_root: /path/to/registered/t2_hist_matched\ndataset_json: /path/to/PI-RADS_data.json\nnum_classes: 4\nmil_mode: att_trans\ntile_count: 24\ntile_size: 64\ndepth: 3\nuse_heatmap: true\nworkers: 4\nepochs: 100\nbatch_size: 8\noptim_lr: 2e-4\nweight_decay: 1e-5\namp: true\nwandb: true\n</code></pre> <pre><code>data_root: /path/to/registered/t2_hist_matched\ndataset_json: /path/to/csPCa_data.json\nnum_classes: 4\nmil_mode: att_trans\ntile_count: 24\ntile_size: 64\ndepth: 3\nuse_heatmap: true\nworkers: 6\ncheckpoint_pirads: /path/to/models/pirads.pt\nepochs: 80\nbatch_size: 8\noptim_lr: 2e-4\n</code></pre> <pre><code>t2_dir: /path/to/raw/t2\ndwi_dir: /path/to/raw/dwi\nadc_dir: /path/to/raw/adc\noutput_dir: /path/to/processed\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#running-tests","title":"Running Tests","text":"<pre><code># Full test suite\npytest tests/\n\n# Single test\npytest tests/test_run.py::test_run_pirads_training\n</code></pre> <p>Tests run in <code>--dry_run</code> mode (2 epochs, batch_size=2, no W&amp;B logging).</p>"},{"location":"contributing/#linting","title":"Linting","text":"<p>This project uses Ruff for linting and formatting:</p> <pre><code># Check for lint errors\nruff check .\n\n# Auto-format code\nruff format .\n</code></pre> <p>Ruff configuration (from <code>pyproject.toml</code>):</p> Setting Value Line length 100 Quote style Double quotes Rules E (errors), W (warnings) Ignored E501 (line too long)"},{"location":"contributing/#slurm-job-scripts","title":"SLURM Job Scripts","text":"<p>Job scripts are in <code>job_scripts/</code> and are configured for GPU partitions:</p> <pre><code>sbatch job_scripts/train_pirads.sh\nsbatch job_scripts/train_cspca.sh\n</code></pre> <p>Key SLURM settings used:</p> Setting Value Partition <code>gpu</code> Memory 128 GB GPUs 1 Time limit 48 hours <p>Tip</p> <p>The SLURM job name (<code>--job-name</code>) automatically becomes the <code>run_name</code>, which determines the log directory at <code>logs/&lt;run_name&gt;/</code>.</p>"},{"location":"contributing/#project-conventions","title":"Project Conventions","text":"<ul> <li>Configs are stored in <code>config/</code> as YAML files</li> <li>Logs are written to <code>logs/&lt;run_name&gt;/</code> including TensorBoard events and training logs</li> <li>Models are saved to <code>logs/&lt;run_name&gt;/</code> during training; best models are saved to <code>models/</code> for deployment</li> <li>Cache is stored at <code>logs/&lt;run_name&gt;/cache/</code> and cleaned up automatically after training</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>For installation and downloading the saved weights of the models, please check the github repository.</p>"},{"location":"getting-started/#data-format","title":"Data Format","text":"<p>Input bpMRI scans should have all three sequnces, T2W, ADC and DWI as separate files in NRRD format. The prostate mask and heatmap are generated by the sample for each sample. To train the models or run testing with reference labels, the data pipeline uses MONAI's decathlon-format JSON:</p> <p><pre><code>{\n    \"train\": [\n        {\n            \"image\": \"path/to/t2.nrrd\",\n            \"dwi\": \"path/to/dwi.nrrd\",\n            \"adc\": \"path/to/adc.nrrd\",\n            \"mask\": \"path/to/prostate_mask.nrrd\",\n            \"heatmap\": \"path/to/heatmap.nrrd\",\n            \"label\": 0\n        }\n    ],\n    \"test\": [...]\n}\n</code></pre> Sample JSON files are available in the github repository. Paths are relative to <code>data_root</code>. PI-RADS labels are 0-indexed (<code>0</code> = PI-RADS 2, <code>3</code> = PI-RADS 5). csPCa labels are binary (0 or 1).</p>"},{"location":"inference/","title":"Inference","text":""},{"location":"inference/#full-pipeline","title":"Full Pipeline","text":"<p><code>run_inference.py</code> runs the complete pipeline: preprocessing followed by PI-RADS classification and csPCa risk prediction.</p> <pre><code>python run_inference.py --config config/config_preprocess.yaml\n</code></pre> <p>This script:</p> <ol> <li>Runs all four preprocessing steps (register, segment, histogram match, heatmap)</li> <li>Loads the PI-RADS model from <code>models/pirads.pt</code></li> <li>Loads the csPCa model from <code>models/cspca_model.pth</code></li> <li>For each scan: predicts PI-RADS score, csPCa risk probability, and identifies the top-5 most-attended patches</li> </ol>"},{"location":"inference/#required-model-files","title":"Required Model Files","text":"<p>Place these in the <code>models/</code> directory:</p> File Description <code>pirads.pt</code> Trained PI-RADS MIL model checkpoint <code>cspca_model.pth</code> Trained csPCa model checkpoint <code>prostate_segmentation_model.pt</code> Pre-trained prostate segmentation model"},{"location":"inference/#output-format","title":"Output Format","text":"<p>Results are saved to <code>&lt;output_dir&gt;/results.json</code>:</p> <pre><code>{\n    \"patient_001.nrrd\": {\n        \"Predicted PIRAD Score\": 4.0,\n        \"csPCa risk\": 0.8234,\n        \"Top left coordinate of top 5 patches(x,y,z)\": [\n            [32, 45, 7],\n            [28, 50, 7],\n            [35, 42, 8],\n            [30, 48, 6],\n            [33, 44, 8]\n        ]\n    }\n}\n</code></pre>"},{"location":"inference/#label-mapping","title":"Label Mapping","text":"<p>PI-RADS predictions are 0-indexed internally and shifted by +2 for display:</p> Internal Label PI-RADS Score 0 PI-RADS 2 1 PI-RADS 3 2 PI-RADS 4 3 PI-RADS 5 <p>csPCa risk is a continuous probability in [0, 1].</p>"},{"location":"inference/#testing-individual-models","title":"Testing Individual Models","text":""},{"location":"inference/#pi-rads-testing","title":"PI-RADS Testing","text":"<pre><code>python run_pirads.py --mode test \\\n    --config config/config_pirads_test.yaml \\\n    --checkpoint models/pirads.pt\n</code></pre> <p>Reports Quadratic Weighted Kappa (QWK) across multiple seeds.</p>"},{"location":"inference/#cspca-testing","title":"csPCa Testing","text":"<pre><code>python run_cspca.py --mode test \\\n    --config config/config_cspca_test.yaml \\\n    --checkpoint_cspca models/cspca_model.pth\n</code></pre> <p>Reports AUC, sensitivity, and specificity with 95% confidence intervals across 20 seeds (default).</p>"},{"location":"pipeline/","title":"Pipeline","text":"<p>The full pipeline has three phases: preprocessing, PI-RADS training (Stage 1), and csPCa training (Stage 2).</p> <pre><code>flowchart TD\n    subgraph Preprocessing\n        R[register_and_crop] --&gt; S[get_segmentation_mask]\n        S --&gt; H[histogram_match]\n        H --&gt; G[get_heatmap]\n    end\n\n    subgraph Stage 1\n        P[PI-RADS Training&lt;br/&gt;CrossEntropy + Attention Loss]\n    end\n\n    subgraph Stage 2\n        C[csPCa Training&lt;br/&gt;Frozen Backbone + BCE Loss]\n    end\n\n    G --&gt; P\n    P --&gt;|frozen backbone| C</code></pre>"},{"location":"pipeline/#preprocessing","title":"Preprocessing","text":"<pre><code>python preprocess_main.py \\\n    --config config/config_preprocess.yaml \\\n    --steps register_and_crop get_segmentation_mask histogram_match get_heatmap\n</code></pre> <p>Run the following steps in sequnce:</p>"},{"location":"pipeline/#step-1-register-and-crop","title":"Step 1: Register and Crop","text":"<p>Resamples T2W, DWI, and ADC to a common spacing of <code>(0.4, 0.4, 3.0)</code> mm using <code>picai_prep</code>, then center-crops with a configurable margin (default 20%).</p>"},{"location":"pipeline/#step-2-prostate-segmentation","title":"Step 2: Prostate Segmentation","text":"<p>Runs a pre-trained segmentation model on T2W images to generate binary prostate masks. Post-processing retains only the top 10 slices by non-zero voxel count.</p>"},{"location":"pipeline/#step-3-histogram-matching","title":"Step 3: Histogram Matching","text":"<p>Matches the histogram intensity of each sequnce to a reference image within masked (prostate) regions using <code>skimage.exposure.match_histograms</code>.</p>"},{"location":"pipeline/#step-4-heatmap-generation","title":"Step 4: Heatmap Generation","text":"<p>Creates weak-attention heatmaps from DWI and ADC:</p> <ul> <li>DWI heatmap: <code>(dwi - min) / (max - min)</code> \u2014 higher DWI signal = higher attention</li> <li>ADC heatmap: <code>(max - adc) / (max - min)</code> \u2014 lower ADC = higher attention (inverted)</li> <li>Combined: element-wise product, re-normalized to [0, 1]</li> </ul> <p>Step Dependencies</p> <p>Steps must run in the order shown above. The pipeline validates dependencies automatically \u2014 for example, <code>get_heatmap</code> requires <code>get_segmentation_mask</code> and <code>histogram_match</code> to have run first.</p>"},{"location":"pipeline/#stage-1-pi-rads-classification","title":"Stage 1: PI-RADS Classification","text":"<p>Trains a 4-class PI-RADS classifier (grades 2\u20135, mapped to labels 0\u20133).</p> <pre><code>python run_pirads.py --mode train --config config/config_pirads_train.yaml\n</code></pre> <p>Training details:</p> Component Value Loss CrossEntropy + cosine-similarity attention loss Attention loss weight Linear warmup over 25 epochs to <code>lambda=2.0</code> Optimizer AdamW (base LR <code>2e-4</code>, transformer LR <code>6e-5</code>) Scheduler CosineAnnealingLR Metric Quadratic Weighted Kappa (QWK) Early stopping After 40 epochs without validation loss improvement AMP Disabled by default (enabled in example YAML config) <p>Attention loss: For each batch, the model's learned attention weights are compared against heatmap-derived attention labels via cosine similarity. PI-RADS 2 samples receive uniform attention (no lesion expected). The loss is weighted by <code>lambda_att</code>, which warms up linearly over the first 25 epochs.</p>"},{"location":"pipeline/#stage-2-cspca-risk-prediction","title":"Stage 2: csPCa Risk Prediction","text":"<p>Builds on a frozen PI-RADS backbone to predict binary csPCa risk. The self-attention and classification head are fine-tuned.</p> <pre><code>python run_cspca.py --mode train --config config/config_cspca_train.yaml\n</code></pre> <p>Training details:</p> Component Value Loss Binary Cross-Entropy (BCE) Backbone Frozen PI-RADS model (ResNet18 + Transformer); attention module is trainable Head SimpleNN: <code>512 \u2192 256 \u2192 128 \u2192 1</code> with ReLU + Dropout(0.3) + Sigmoid Optimizer AdamW (LR <code>2e-4</code>) Seeds 20 random seeds (default) for 95% CI Metrics AUC, Sensitivity, Specificity <p>The backbone's feature extractor (<code>net</code>), transformer, and <code>myfc</code> are frozen. The attention module and <code>SimpleNN</code> classification head are trained. After training the framework reports mean and 95% confidence intervals for AUC, sensitivity, and specificity by testing across 20 random seeds.</p> <p>Refer to Getting Started for JSON dataset format to run run_pirads.py and run_cspca.py</p>"},{"location":"api/data/","title":"Data Loading Reference","text":""},{"location":"api/data/#get_dataloader","title":"get_dataloader","text":"<pre><code>def get_dataloader(args, split: Literal[\"train\", \"test\"]) -&gt; DataLoader\n</code></pre> <p>Creates a PyTorch DataLoader with MONAI transforms and persistent caching.</p> <p>Parameters:</p> Parameter Description <code>args</code> Namespace with <code>dataset_json</code>, <code>data_root</code>, <code>tile_size</code>, <code>tile_count</code>, <code>depth</code>, <code>use_heatmap</code>, <code>batch_size</code>, <code>workers</code>, <code>dry_run</code>, <code>logdir</code> <code>split</code> <code>\"train\"</code> or <code>\"test\"</code> <p>Behavior:</p> <ul> <li>Loads data lists from a MONAI decathlon-format JSON</li> <li>In <code>dry_run</code> mode, limits to 8 samples</li> <li>Uses <code>PersistentDataset</code> with cache stored at <code>&lt;logdir&gt;/cache/&lt;split&gt;/</code></li> <li>Training split is shuffled; test split is not</li> <li>Uses <code>list_data_collate</code> to stack patches into <code>[B, N, C, D, H, W]</code></li> </ul>"},{"location":"api/data/#transform-pipeline","title":"Transform Pipeline","text":"<p>Two variants depending on <code>args.use_heatmap</code>:</p>"},{"location":"api/data/#with-heatmaps-default","title":"With Heatmaps (default)","text":"Step Transform Description 1 <code>LoadImaged</code> Load T2, mask, DWI, ADC, heatmap (ITKReader, channel-first) 2 <code>ClipMaskIntensityPercentilesd</code> Clip T2 intensity to [0, 99.5] percentiles within mask 3 <code>ConcatItemsd</code> Stack T2 + DWI + ADC \u2192 3-channel image 4 <code>NormalizeIntensity_customd</code> Z-score normalize per channel using mask-only statistics 5 <code>ElementwiseProductd</code> Multiply mask * heatmap \u2192 <code>final_heatmap</code> 6 <code>RandWeightedCropd</code> Extract N patches weighted by <code>final_heatmap</code> 7 <code>EnsureTyped</code> Cast labels to float32 8 <code>Transposed</code> Reorder image dims for 3D convolution 9 <code>DeleteItemsd</code> Remove intermediate keys (mask, dwi, adc, heatmap) 10 <code>ToTensord</code> Convert to PyTorch tensors"},{"location":"api/data/#without-heatmaps","title":"Without Heatmaps","text":"Step Transform Description 1 <code>LoadImaged</code> Load T2, mask, DWI, ADC 2 <code>ClipMaskIntensityPercentilesd</code> Clip T2 intensity to [0, 99.5] percentiles within mask 3 <code>ConcatItemsd</code> Stack T2 + DWI + ADC \u2192 3-channel image 4 <code>NormalizeIntensityd</code> Standard channel-wise normalization (MONAI built-in) 5 <code>RandCropByPosNegLabeld</code> Extract N patches from positive (mask) regions 6 <code>EnsureTyped</code> Cast labels to float32 7 <code>Transposed</code> Reorder image dims 8 <code>DeleteItemsd</code> Remove intermediate keys 9 <code>ToTensord</code> Convert to tensors"},{"location":"api/data/#list_data_collate","title":"list_data_collate","text":"<pre><code>def list_data_collate(batch: Sequence) -&gt; dict\n</code></pre> <p>Custom collation function that stacks per-patient patch lists into batch tensors.</p> <p>Each sample from the dataset is a list of N patch dictionaries. This function:</p> <ol> <li>Stacks <code>image</code> across patches: <code>[N, C, D, H, W]</code> per sample</li> <li>Stacks <code>final_heatmap</code> if present</li> <li>Applies PyTorch's <code>default_collate</code> to form the batch dimension</li> </ol> <p>Result: <code>{\"image\": [B, N, C, D, H, W], \"label\": [B], ...}</code></p>"},{"location":"api/data/#custom-transforms","title":"Custom Transforms","text":""},{"location":"api/data/#clipmaskintensitypercentilesd","title":"ClipMaskIntensityPercentilesd","text":"<pre><code>ClipMaskIntensityPercentilesd(\n    keys: KeysCollection,\n    mask_key: str,\n    lower: float | None,\n    upper: float | None,\n    sharpness_factor: float | None = None,\n    channel_wise: bool = False,\n    dtype: DtypeLike = np.float32,\n)\n</code></pre> <p>Clips image intensity to percentiles computed only from the masked region. Supports both hard clipping (default) and soft clipping (via <code>sharpness_factor</code>).</p>"},{"location":"api/data/#normalizeintensity_customd","title":"NormalizeIntensity_customd","text":"<pre><code>NormalizeIntensity_customd(\n    keys: KeysCollection,\n    mask_key: str,\n    subtrahend: NdarrayOrTensor | None = None,\n    divisor: NdarrayOrTensor | None = None,\n    nonzero: bool = False,\n    channel_wise: bool = False,\n    dtype: DtypeLike = np.float32,\n)\n</code></pre> <p>Z-score normalization where mean and standard deviation are computed only from masked voxels. Supports channel-wise normalization.</p>"},{"location":"api/data/#elementwiseproductd","title":"ElementwiseProductd","text":"<pre><code>ElementwiseProductd(\n    keys: KeysCollection,\n    output_key: str,\n)\n</code></pre> <p>Computes the element-wise product of two arrays from the data dictionary and stores the result in <code>output_key</code>. Used to combine the prostate mask with the attention heatmap.</p>"},{"location":"api/data/#dataset-json-format","title":"Dataset JSON Format","text":"<p>The pipeline expects a MONAI decathlon-format JSON file:</p> <pre><code>{\n    \"train\": [\n        {\n            \"image\": \"relative/path/to/t2.nrrd\",\n            \"dwi\": \"relative/path/to/dwi.nrrd\",\n            \"adc\": \"relative/path/to/adc.nrrd\",\n            \"mask\": \"relative/path/to/mask.nrrd\",\n            \"heatmap\": \"relative/path/to/heatmap.nrrd\",\n            \"label\": 2\n        }\n    ],\n    \"test\": [...]\n}\n</code></pre> <p>Paths are relative to <code>data_root</code>. The <code>heatmap</code> key is only required when <code>use_heatmap=True</code>.</p>"},{"location":"api/models/","title":"Models Reference","text":""},{"location":"api/models/#milmodel_3d","title":"MILModel_3D","text":"<pre><code>class MILModel_3D(nn.Module):\n    def __init__(\n        self,\n        num_classes: int,\n        mil_mode: str = \"att\",\n        pretrained: bool = True,\n        backbone: str | nn.Module | None = None,\n        backbone_num_features: int | None = None,\n        trans_blocks: int = 4,\n        trans_dropout: float = 0.0,\n    )\n</code></pre> <p>Constructor arguments:</p> Argument Type Default Description <code>num_classes</code> <code>int</code> \u2014 Number of output classes <code>mil_mode</code> <code>str</code> <code>\"att\"</code> MIL aggregation mode <code>pretrained</code> <code>bool</code> <code>True</code> Use pretrained backbone weights <code>backbone</code> <code>str \\| nn.Module \\| None</code> <code>None</code> Backbone CNN (None = ResNet18-3D) <code>backbone_num_features</code> <code>int \\| None</code> <code>None</code> Output features of custom backbone <code>trans_blocks</code> <code>int</code> <code>4</code> Number of transformer encoder layers <code>trans_dropout</code> <code>float</code> <code>0.0</code> Transformer dropout rate <p>MIL modes:</p> Mode Description <code>mean</code> Average logits across all patches \u2014 equivalent to pure CNN <code>max</code> Keep only the max-probability instance for loss <code>att</code> Attention-based MIL (Ilse et al., 2018) <code>att_trans</code> Transformer + attention MIL (Shao et al., 2021) <code>att_trans_pyramid</code> Pyramid transformer using intermediate ResNet layers <p>Key methods:</p> <ul> <li><code>forward(x, no_head=False)</code> \u2014 Full forward pass. If <code>no_head=True</code>, returns patch-level features <code>[B, N, 512]</code> before transformer and attention pooling (used during attention loss computation).</li> <li><code>calc_head(x)</code> \u2014 Applies the MIL aggregation and classification head to patch features.</li> </ul> <p>Example:</p> <pre><code>import torch\nfrom src.model.MIL import MILModel_3D\n\nmodel = MILModel_3D(num_classes=4, mil_mode=\"att_trans\")\n# Input: [batch, patches, channels, depth, height, width]\nx = torch.randn(2, 24, 3, 3, 64, 64)\nlogits = model(x)  # [2, 4]\n</code></pre>"},{"location":"api/models/#cspca_model","title":"csPCa_Model","text":"<pre><code>class csPCa_Model(nn.Module):\n    def __init__(self, backbone: nn.Module)\n</code></pre> <p>Wraps a pre-trained <code>MILModel_3D</code> backbone for binary csPCa prediction. The backbone's feature extractor, transformer, and attention mechanism are reused. The original classification head (<code>myfc</code>) is replaced by a <code>SimpleNN</code>.</p> <p>Attributes:</p> Attribute Type Description <code>backbone</code> <code>MILModel_3D</code> Frozen PI-RADS backbone <code>fc_cspca</code> <code>SimpleNN</code> Binary classification head <code>fc_dim</code> <code>int</code> Feature dimension (512 for ResNet18) <p>Example:</p> <pre><code>import torch\nfrom src.model.MIL import MILModel_3D\nfrom src.model.csPCa_model import csPCa_Model\n\nbackbone = MILModel_3D(num_classes=4, mil_mode=\"att_trans\")\nmodel = csPCa_Model(backbone=backbone)\n\nx = torch.randn(2, 24, 3, 3, 64, 64)\nprob = model(x)  # [2, 1] \u2014 sigmoid probabilities\n</code></pre>"},{"location":"api/models/#simplenn","title":"SimpleNN","text":"<pre><code>class SimpleNN(nn.Module):\n    def __init__(self, input_dim: int)\n</code></pre> <p>A lightweight MLP for binary classification:</p> <pre><code>Linear(input_dim, 256) \u2192 ReLU\nLinear(256, 128) \u2192 ReLU \u2192 Dropout(0.3)\nLinear(128, 1) \u2192 Sigmoid\n</code></pre> <p>Input: <code>[B, input_dim]</code> \u2014 Output: <code>[B, 1]</code> (probability).</p>"},{"location":"api/preprocessing/","title":"Preprocessing Reference","text":""},{"location":"api/preprocessing/#overview","title":"Overview","text":"<p>Preprocessing is orchestrated by <code>preprocess_main.py</code>, which runs steps in sequence. Each step receives and returns the <code>args</code> namespace, updating directory paths as it goes.</p>"},{"location":"api/preprocessing/#step-dependencies","title":"Step Dependencies","text":"Step Requires <code>register_and_crop</code> \u2014 <code>get_segmentation_mask</code> <code>register_and_crop</code> <code>histogram_match</code> <code>register_and_crop</code>, <code>get_segmentation_mask</code> <code>get_heatmap</code> <code>register_and_crop</code>, <code>get_segmentation_mask</code>, <code>histogram_match</code> <p>Dependencies are validated at runtime \u2014 the pipeline will exit with an error if steps are out of order.</p>"},{"location":"api/preprocessing/#register_files","title":"register_files","text":"<pre><code>def register_files(args) -&gt; args\n</code></pre> <p>Registers and crops T2, DWI, and ADC images to a standardized spacing and size.</p> <p>Process:</p> <ol> <li>Reads images from <code>args.t2_dir</code>, <code>args.dwi_dir</code>, <code>args.adc_dir</code></li> <li>Resamples to spacing <code>(0.4, 0.4, 3.0)</code> mm using <code>picai_prep.Sample</code></li> <li>Center-crops with <code>args.margin</code> (default 0.2) in x/y dimensions</li> <li>Saves to <code>&lt;output_dir&gt;/t2_registered/</code>, <code>DWI_registered/</code>, <code>ADC_registered/</code></li> </ol> <p>Updates <code>args</code>: <code>t2_dir</code>, <code>dwi_dir</code>, <code>adc_dir</code> \u2192 registered directories.</p>"},{"location":"api/preprocessing/#get_segmask","title":"get_segmask","text":"<pre><code>def get_segmask(args) -&gt; args\n</code></pre> <p>Generates prostate segmentation masks from T2W images using a pre-trained model.</p> <p>Process:</p> <ol> <li>Loads model config from <code>&lt;project_dir&gt;/config/inference.json</code></li> <li>Loads checkpoint from <code>&lt;project_dir&gt;/models/prostate_segmentation_model.pt</code></li> <li>Applies MONAI transforms: orientation (RAS), spacing (0.5 mm isotropic), intensity normalization</li> <li>Runs inference and inverts transforms to original space</li> <li>Post-processes: retains only top 10 slices by non-zero voxel count</li> <li>Saves NRRD masks to <code>&lt;output_dir&gt;/prostate_mask/</code></li> </ol> <p>Updates <code>args</code>: adds <code>seg_dir</code>.</p>"},{"location":"api/preprocessing/#histmatch","title":"histmatch","text":"<pre><code>def histmatch(args) -&gt; args\n</code></pre> <p>Matches the intensity histogram of each modality to a reference image.</p> <p>Process:</p> <ol> <li>Reads reference images from <code>&lt;project_dir&gt;/dataset/</code> (<code>t2_reference.nrrd</code>, <code>dwi_reference.nrrd</code>, <code>adc_reference.nrrd</code>, <code>prostate_segmentation_reference.nrrd</code>)</li> <li>For each patient, matches histograms within the prostate mask using <code>skimage.exposure.match_histograms</code></li> <li>Saves to <code>&lt;output_dir&gt;/t2_histmatched/</code>, <code>DWI_histmatched/</code>, <code>ADC_histmatched/</code></li> </ol> <p>Updates <code>args</code>: <code>t2_dir</code>, <code>dwi_dir</code>, <code>adc_dir</code> \u2192 histogram-matched directories.</p>"},{"location":"api/preprocessing/#get_histmatched","title":"get_histmatched","text":"<pre><code>def get_histmatched(\n    data: np.ndarray,\n    ref_data: np.ndarray,\n    mask: np.ndarray,\n    ref_mask: np.ndarray,\n) -&gt; np.ndarray\n</code></pre> <p>Low-level function that performs histogram matching on masked regions only. Unmasked pixels remain unchanged.</p>"},{"location":"api/preprocessing/#get_heatmap","title":"get_heatmap","text":"<pre><code>def get_heatmap(args) -&gt; args\n</code></pre> <p>Generates combined DWI/ADC attention heatmaps.</p> <p>Process:</p> <ol> <li>For each file, reads DWI, ADC, and prostate mask</li> <li>Computes DWI heatmap: <code>(dwi - min) / (max - min)</code> within mask</li> <li>Computes ADC heatmap: <code>(max - adc) / (max - min)</code> within mask (inverted \u2014 low ADC = high attention)</li> <li>Combines via element-wise multiplication</li> <li>Re-normalizes to [0, 1]</li> <li>Saves to <code>&lt;output_dir&gt;/heatmaps/</code></li> </ol> <p>Updates <code>args</code>: adds <code>heatmapdir</code>.</p> <p>Edge cases</p> <p>If all values within the mask are identical for a modality (DWI or ADC), that modality's heatmap is skipped. If both are constant, the heatmap defaults to all ones.</p>"}]}